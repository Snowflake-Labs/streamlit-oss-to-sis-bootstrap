{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "apunkdimx4e6pcz7qb6g",
   "authorId": "4227028274930",
   "authorName": "KAMESHS",
   "authorEmail": "kamesh.sampath@hotmail.com",
   "sessionId": "a7872877-3787-46d3-a9ba-f3f46c52aa70",
   "lastEditTime": 1737106813049
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "md_setup_intro",
    "resultHeight": 311,
    "collapsed": false
   },
   "source": "# Preparing Snowflake\nThis note book will guide you through the required steps thats need to prepare your Snowflake account to deploy the demo [Streamlit ML App](https://github.com/kameshsampath/st-ml-app)\n\nTypically we will\n\n- [x] Create Schemas and Tables\n- [x] Create a external stage to S3\n- [x] Load Penguins Dataset on the table\n- [x] Create Snowpark Contiainer Services(SPCS) objects",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "id": "2568d714-4541-4058-8bf1-7b0cbbdc7ab5",
   "metadata": {
    "name": "md_setup_schemas",
    "collapsed": false,
    "resultHeight": 342
   },
   "source": "## Schemas and Stages\n\nIn the next cell we will create the schemas and stages that will be used in this tutorial.\n\n### Schemas\n\n|Schema | Use|\n|------- |----------------|\n| apps | Will hold all applications e.g. Streamlit|\n| data | Will hold all data tables  |\n| stages | All  stages |\n| file_formats | All  file formats that will be used during data load |\n\n"
  },
  {
   "cell_type": "code",
   "id": "b28d9a98-b375-4db9-8219-0df97eb39f14",
   "metadata": {
    "language": "sql",
    "name": "sql_create_schemas",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "-- data schema\nCREATE SCHEMA IF NOT EXISTS DATA;\n-- create schema to hold all stages\nCREATE SCHEMA IF NOT EXISTS STAGES;\n-- create schema to hold all file formats\nCREATE SCHEMA IF NOT EXISTS FILE_FORMATS;\n-- apps to hold all streamlit apps\nCREATE SCHEMA IF NOT EXISTS APPS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc1af8cf-344c-46c5-9007-4b2dd0c28761",
   "metadata": {
    "name": "md_external_stage",
    "collapsed": false,
    "resultHeight": 114
   },
   "source": "\n### Stages and File Format\n\nWe will create stage named `stages.st_ml_app_penguins` which will point to an s3 bucket `s3://sfquickstarts/misc` and it will use the file file format `file_formats.csv` to parse and laod CSV files."
  },
  {
   "cell_type": "code",
   "id": "579e0531-4a76-493b-9089-22e0b4a59e6f",
   "metadata": {
    "language": "sql",
    "name": "sql_create_stage",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "-- add an external stage to a s3 bucket\nCREATE STAGE IF NOT EXISTS STAGES.ST_ML_APP_PENGUINS\n  URL='s3://sfquickstarts/misc';\n\n-- default CSV file format and allow values to quoted by \"\nCREATE FILE FORMAT IF NOT EXISTS FILE_FORMATS.CSV\n  TYPE='CSV'\n  SKIP_HEADER=1\n  FIELD_OPTIONALLY_ENCLOSED_BY = '\"';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7c9a1b2-01eb-4430-80dd-ad895d96a530",
   "metadata": {
    "name": "md_load_data",
    "collapsed": false,
    "resultHeight": 128
   },
   "source": "## Load \n\nAs part of next step let us load the data on `data.penguins` table using the file from `@stages.st_ml_app_penguins/penguins_cleaned.csv`"
  },
  {
   "cell_type": "code",
   "id": "3f387e15-1bfe-46a5-9ed8-7000d75afeef",
   "metadata": {
    "language": "sql",
    "name": "load_penguins_data",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Create table to hold penguins data\nCREATE OR ALTER TABLE DATA.PENGUINS(\n   SPECIES STRING NOT NULL,\n   ISLAND STRING NOT NULL,\n   BILL_LENGTH_MM NUMBER NOT NULL,\n   BILL_DEPTH_MM NUMBER NOT NULL,\n   FLIPPER_LENGTH_MM NUMBER NOT NULL,\n   BODY_MASS_G NUMBER NOT NULL,\n   SEX STRING NOT NULL\n);\n\n-- Load the data from penguins_cleaned.csv\nCOPY INTO DATA.PENGUINS\nFROM @stages.st_ml_app_penguins/penguins_cleaned.csv\nFILE_FORMAT=(FORMAT_NAME='FILE_FORMATS.CSV');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd53f3db-b787-4343-8726-2c5e8322d106",
   "metadata": {
    "name": "md_query_data",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let us select and verify the data,"
  },
  {
   "cell_type": "code",
   "id": "13ce4fc6-d28b-463f-85cf-b8dc42ae2fc0",
   "metadata": {
    "language": "python",
    "name": "py_query_data",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\ndf = session.table('st_ml_app.data.penguins')\ndf.show(10)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82130382-e0f5-4603-baf2-f9279e0519a8",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 128
   },
   "source": "## Telemetry Settings\nIn the following steps, we will set up Telemetry Events (logs/traces) at the database level. While Snowflake defaults to storing events in `SNOWFLAKE.TELEMETRY.EVENTS`, for this demo we will configure event collection at the database level."
  },
  {
   "cell_type": "markdown",
   "id": "2e10a335-31d7-4a91-8d3d-15f015b70919",
   "metadata": {
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Create the event table at the database level and set it as the default Events table for the database."
  },
  {
   "cell_type": "code",
   "id": "5182bd52-6f1e-4def-9fbf-fa7876bbf834",
   "metadata": {
    "language": "sql",
    "name": "sql_setup_telemetry",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "-- create schema\nCREATE SCHEMA IF NOT EXISTS ST_ML_APP.telemetry;\n-- create event table \nCREATE EVENT TABLE IF NOT EXISTS ST_ML_APP.telemetry.events;\n-- set to new event table\nALTER DATABASE ST_ML_APP SET EVENT_TABLE = ST_ML_APP.telemetry.events;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58b76653-f6ed-4d58-955f-185fade3206e",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Create the event table at the database level and set it as the default Events table for the database."
  },
  {
   "cell_type": "code",
   "id": "5e80b5f1-2ed6-462f-8862-6618adc924c6",
   "metadata": {
    "language": "sql",
    "name": "set_telemetry_levels",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "-- set log, trace and metrtic levels\nALTER DATABASE ST_ML_APP SET LOG_LEVEL = DEBUG;\nALTER DATABASE ST_ML_APP SET TRACE_LEVEL = ALWAYS;\nALTER DATABASE ST_ML_APP SET METRIC_LEVEL = ALL;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cf3d2b8-e138-46f8-9436-21e11f00fc35",
   "metadata": {
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 124
   },
   "source": "> **IMPORTANT:**\n>\n> The following sections are required only if you are doing SPCS sections of the lab. \n> SPCS is not available for enterprise trials."
  },
  {
   "cell_type": "markdown",
   "id": "93a1b2dd-ed75-4d2c-bbb5-41b1517ac8fa",
   "metadata": {
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Snowpark Container Services(SPCS)"
  },
  {
   "cell_type": "code",
   "id": "9a70c85c-5780-474d-8618-2abcc236bd4f",
   "metadata": {
    "language": "sql",
    "name": "sql_current_user_role",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "SELECT current_role() as current_role",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80ccf785-40a9-40bd-9d51-d0fe017b5f62",
   "metadata": {
    "language": "sql",
    "name": "sql_current_database",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "SELECT current_database() as current_database",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9effe51-b4cb-4ea9-ade6-098fc5088977",
   "metadata": {
    "language": "sql",
    "name": "sql_current_user",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "SELECT current_user() as current_user",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777f718d-6348-44d9-9434-6e764f76fe35",
   "metadata": {
    "language": "python",
    "name": "spcs_variables",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "__current_role=sql_current_user_role.to_pandas().iloc[0]['CURRENT_ROLE']\n__current_db=sql_current_database.to_pandas().iloc[0]['CURRENT_DATABASE']\n__current_user=sql_current_user.to_pandas().iloc[0]['CURRENT_USER']\n__spcs_st_ml_app_role='st_ml_app'\n__spcs_role_name=__current_role\n__spcs_images_schema='images'\n__spcs_wh_name='st_ml_app_spcs_wh_s'\n__spcs_compute_pool='st_ml_app_xs'\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f73a510b-1534-450d-ab8f-736afcb70e93",
   "metadata": {
    "language": "sql",
    "name": "spcs_objects",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Grant ownership on the DB to ACCOUNTADMIN as we will be creating few other objects which might require higher privileges\n-- If you have created all objects as ACCOUNTADMIN then you can skip this step i.e `__current_role` is ACCOUNTADMIN\nGRANT OWNERSHIP ON DATABASE {{__current_db}} TO ROLE ACCOUNTADMIN COPY CURRENT GRANTS;\n\nUSE ROLE ACCOUNTADMIN;\n\n-- Role that will be used to create services\nCREATE ROLE IF NOT EXISTS {{__spcs_st_ml_app_role}};\n\n-- grant {{__spcs_st_ml_app_role}} role to current user\nGRANT ROLE {{__spcs_st_ml_app_role}} TO USER {{__current_user}};\n\n-- Grant ownership on the DB to {{__spcs_st_ml_app_role}}\nGRANT OWNERSHIP ON DATABASE {{__current_db}} TO ROLE {{__spcs_st_ml_app_role}} COPY CURRENT GRANTS;\n\n-- use {{__spcs_st_ml_app_role}} to create the data schema\nUSE ROLE {{__spcs_st_ml_app_role}};\n\n-- data_schema will house the image repositories\nCREATE SCHEMA IF NOT EXISTS {{__current_db}}.{{__spcs_images_schema}};\nGRANT OWNERSHIP ON SCHEMA {{__current_db}}.{{__spcs_images_schema}} TO ROLE {{__spcs_st_ml_app_role}} ;\n-- grant __spcs_st_ml_app_role to own other schemas too to allow the Streamlit app within\n-- the container to access the tables\nGRANT OWNERSHIP ON SCHEMA {{__current_db}}.DATA TO ROLE {{__spcs_st_ml_app_role}} ;\nGRANT OWNERSHIP ON TABLE {{__current_db}}.DATA.PENGUINS TO ROLE {{__spcs_st_ml_app_role}} ;\n\n-- Switch back to accountadmin for rest of the tasks\nUSE ROLE ACCOUNTADMIN;\n\n-- Create warehouse to be used for queries from the service\nCREATE WAREHOUSE IF NOT EXISTS {{__spcs_wh_name}} WITH\n  WAREHOUSE_SIZE='X-SMALL'\n  -- disable auto start\n  INITIALLY_SUSPENDED=TRUE\n  -- auto suspend in two mins\n  AUTO_SUSPEND=120;\n\n-- grants on warehouse to {{__spcs_st_ml_app_role}}\nGRANT USAGE ON WAREHOUSE {{__spcs_wh_name}} TO ROLE {{__spcs_st_ml_app_role}};\n\n-- allow endpoint binding to role \nGRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE {{__spcs_st_ml_app_role}};\n\n-- allow role to use and monitor compute pool\nGRANT USAGE, MONITOR ON COMPUTE POOL {{__spcs_compute_pool}} TO ROLE {{__spcs_st_ml_app_role}};",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df1cf082-d999-411d-9d68-43bf1e484cd8",
   "metadata": {
    "name": "md_spcs_cleanup",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Cleanup\nSPCS has compute associated with it, run the following cell to clean the Snowflake resources created as part of the demo."
  },
  {
   "cell_type": "code",
   "id": "87ac04b2-6618-4dd8-8c96-7ce228cd763e",
   "metadata": {
    "language": "sql",
    "name": "spcs_cleanup",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- drop the service\nDROP SERVICE IF EXISTS {{__current_db}}.{{__spcs_images_schema}}.ST_ML_APP;\n\nUSE ROLE ACCOUNTADMIN;\n\n-- gracefully stop and delete all services running on this compute pool\nALTER COMPUTE POOL {{__spcs_compute_pool}} STOP ALL;\n\nDROP COMPUTE POOL IF EXISTS {{__spcs_compute_pool}};\n\n-- drop role \nDROP ROLE IF EXISTS {{__spcs_st_ml_app_role}};\n\n-- suspend warehouse \nALTER WAREHOUSE {{__spcs_wh_name}} SUSPEND;",
   "execution_count": null
  }
 ]
}