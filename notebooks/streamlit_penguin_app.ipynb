{
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "md_intro",
    "resultHeight": 535,
    "collapsed": false
   },
   "source": "# Building an Interactive Machine Learning Demo with Streamlit in Snowflake\n\nIn this notebook, we'll create and deploy an interactive Machine Learning application using Streamlit, running it entirely within a Snowflake Notebook environment. This hands-on exercise will demonstrate how to combine the power of Streamlit's user interface capabilities with scikit-learn's machine learning tools.\n\n## Learning Objectives\n\nBy completing this exercise, you will:\n\n- Master the usage of Streamlit widgets to create interactive data applications\n- Deploy and run a Streamlit application within Snowflake Notebook\n- Implement a practical classification model using scikit-learn\n- Create interactive ML predictions using Streamlit's dynamic interface capabilities\n\nThe unique aspect of this tutorial is that everything runs directly within your Snowflake Notebook environment, providing a seamless development experience.\n\n- Reference Implementation: [Streamlit Machine Learning Demo](https://github.com/kameshsampath/st-ml-app)\n- Detailed Tutorial: [Zero to Streamlit](https://snowflake-labs.github.io/zero-to-streamlit/) - A comprehensive guide by Snowflake Developers on building Streamlit applications\n",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "id": "cea10b02-7b79-4fb4-8f08-5d58f6398ee8",
   "metadata": {
    "name": "md_pre_req",
    "collapsed": false,
    "resultHeight": 623
   },
   "source": "## Pre-requisite\n\nBefore we dive into building our Machine Learning application, this notebook will guide you through the essential setup steps required to prepare your Snowflake account. These preparations are crucial for deploying and running the Streamlit ML App successfully.\n\n## Setup Steps\n\nWe will complete the following configuration tasks:\n\n1. Database Structure Setup\n\n   - Create necessary schemas\n   - Set up required tables for our ML application\n\n\n2. External Storage Configuration\n\n   - Create and configure an external stage connected to Amazon S3 \n   - Establish secure data access pathways\n\n3. Data Preparation\n\n   - Load the Penguins dataset into Snowflake\n   - Prepare the data structure for ML operations\n\nThis foundational setup will ensure smooth execution of our Machine Learning application within the Snowflake environment. \n\nLet's proceed with these prerequisites step by step."
  },
  {
   "cell_type": "markdown",
   "id": "34c3b93e-b674-4603-a68e-8f0fd3c2e2f7",
   "metadata": {
    "name": "md_env_schemas",
    "collapsed": false,
    "resultHeight": 596
   },
   "source": "\n## Environment Setup: Schemas and Stages\n\nIn this section, we'll establish the foundational database structures needed for our Streamlit ML application. We'll create dedicated schemas to ensure proper organization and separation of concerns.\n\n## Schema Organization\n\n| Schema | Purpose |\n|--------|----------|\n| `apps` | Houses all application components, specifically our Streamlit application |\n| `data` | Stores all data tables, including our Penguins dataset |\n| `stages` | Contains all staging areas for data loading and file management |\n| `file_formats` | Defines the file formats used for data ingestion |\n\nEach schema serves a specific purpose in our application architecture:\n- The `apps` schema keeps our application code isolated\n- The `data` schema maintains our datasets in an organized manner\n- The `stages` schema manages our external connections\n- The `file_formats` schema ensures consistent data loading formats\n\nLet's proceed with creating these schemas in our Snowflake environment."
  },
  {
   "cell_type": "code",
   "id": "0019ab10-21cf-493d-b328-ab8f836d7844",
   "metadata": {
    "language": "sql",
    "name": "sql_schemas"
   },
   "outputs": [],
   "source": "-- data schema\nCREATE SCHEMA IF NOT EXISTS DATA;\n-- create schema to hold all stages\nCREATE SCHEMA IF NOT EXISTS STAGES;\n-- create schema to hold all file formats\nCREATE SCHEMA IF NOT EXISTS FILE_FORMATS;\n-- apps to hold all streamlit apps\nCREATE SCHEMA IF NOT EXISTS APPS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11bdb7e7-20da-42de-a27e-f074ea90962f",
   "metadata": {
    "name": "md_env_stages",
    "collapsed": false,
    "resultHeight": 398
   },
   "source": "## Stage and File Format Configuration\n\nIn this section, we'll set up the necessary staging area and file format for our data loading process. Specifically, we will:\n\n1. Create a stage named `stages.st_ml_app_penguins` that will:\n   - Connect to the S3 bucket `s3://sfquickstarts/misc`\n   - Serve as our data loading pipeline\n\n2. Configure a file format `file_formats.csv` that will:\n   - Define how we parse and load CSV files\n   - Be associated with our stage for data processing\n\nThis setup will establish the foundation for loading our Penguins dataset into Snowflake.\n\nLet's proceed with creating these configurations...\n"
  },
  {
   "cell_type": "code",
   "id": "8c4b0e50-0df8-42bc-a512-a8be6155020e",
   "metadata": {
    "language": "sql",
    "name": "sql_stages"
   },
   "outputs": [],
   "source": "-- add an external stage to a s3 bucket\nCREATE STAGE IF NOT EXISTS STAGES.ST_ML_APP_PENGUINS\n  URL='s3://sfquickstarts/misc';\n\n-- default CSV file format and allow values to quoted by \"\nCREATE FILE FORMAT IF NOT EXISTS FILE_FORMATS.CSV\n  TYPE='CSV'\n  SKIP_HEADER=1\n  FIELD_OPTIONALLY_ENCLOSED_BY = '\"';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8fb9e105-5d23-48dc-b66d-2a807a50a03d",
   "metadata": {
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 513
   },
   "source": "## Loading the Penguins Dataset\n\nAs our next step, we'll load the penguins dataset that will serve as the foundation for our ML demo application. The dataset contains various measurements of different penguin species, making it perfect for our classification tasks.\n\n## Data Loading Process\n\nWe will:\n- Create a table `data.penguins` to store our penguin details\n- Load data from the file `penguins_cleaned.csv` located in our external stage\n- Use the previously configured stage path: `@stages.st_ml_app_penguins/penguins_cleaned.csv`\n\nThis dataset will be used throughout our demo to:\n- Train our machine learning model\n- Make predictions on penguin species\n- Demonstrate interactive data visualization\n\nLet's proceed with the data loading commands..."
  },
  {
   "cell_type": "code",
   "id": "c0d35b0f-638c-45b9-a026-4cead0159f8e",
   "metadata": {
    "language": "sql",
    "name": "sql_tables"
   },
   "outputs": [],
   "source": "-- Create table to hold penguins data\nCREATE OR ALTER TABLE DATA.PENGUINS(\n   SPECIES STRING NOT NULL,\n   ISLAND STRING NOT NULL,\n   BILL_LENGTH_MM NUMBER NOT NULL,\n   BILL_DEPTH_MM NUMBER NOT NULL,\n   FLIPPER_LENGTH_MM NUMBER NOT NULL,\n   BODY_MASS_G NUMBER NOT NULL,\n   SEX STRING NOT NULL\n);\n\n-- Load the data from penguins_cleaned.csv\nCOPY INTO DATA.PENGUINS\nFROM @STAGES.ST_ML_APP_PENGUINS/PENGUINS_CLEANED.CSV\nFILE_FORMAT=(FORMAT_NAME='FILE_FORMATS.CSV');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa430827-c3b5-4be8-b90b-79d9443a1ab4",
   "metadata": {
    "name": "md_app_intro",
    "collapsed": false,
    "resultHeight": 513
   },
   "source": "## Building Our Streamlit ML Application\n\nNow that we have our environment set up and the penguins dataset loaded, let's start building our interactive Machine Learning application using Streamlit. We'll create a user-friendly interface that allows users to:\n\n- Visualize the penguins dataset\n- Input penguin measurements through interactive widgets\n- Make real-time predictions using our trained ML model\n- Display the results in an engaging way\n\n### Getting Started\nWe'll begin by importing the necessary libraries and setting up our Streamlit application structure. Our app will leverage:\n- Streamlit for the interactive web interface\n- scikit-learn for our ML model\n- Snowflake for data access\n- Pandas for data manipulation\n\nLet's dive into the code and build our application step by step..."
  },
  {
   "cell_type": "code",
   "id": "384139f1-3cfd-44bc-ae55-c2c4ffde00fa",
   "metadata": {
    "language": "python",
    "name": "py_imports",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "import streamlit as st\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StringType, DecimalType",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3067b115-7810-4dbd-bbb5-ddb475fee01c",
   "metadata": {
    "name": "md_about_session",
    "collapsed": false,
    "resultHeight": 291
   },
   "source": "### Application Session Configuration\n\nThe following code demonstrates a flexible session management approach that enables our Streamlit application to run seamlessly in both local development and Snowflake environments. This dual-environment capability is crucial for:\n\n#### Key Benefits\n- Development flexibility: Test and debug on your local machine\n- Production readiness: Deploy directly to Snowflake\n- Consistent behavior: Same application code works in both environments\n- Efficient development cycle: Quick iterations during development\n"
  },
  {
   "cell_type": "code",
   "id": "2b9c1693-441b-433b-b157-8ba911ece77d",
   "metadata": {
    "language": "python",
    "name": "py_sf_session",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "def get_active_session():\n    \"\"\"\n    Returns the active Snowflake session based on the environment.\n    For local development, it creates a new connection.\n    For Snowflake, it uses the existing session.\n    \"\"\"\n    conn = st.connection(\n        os.getenv(\n            \"SNOWFLAKE_CONNECTION_NAME\",\n            \"devrel-ent\",\n        ),\n        type=\"snowflake\",\n    )\n    return conn.session()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28fb1802-478d-4b4b-99fd-a387a34bbbbc",
   "metadata": {
    "name": "md_session_adv",
    "collapsed": false,
    "resultHeight": 140
   },
   "source": "This session management function is a cornerstone of our application's architecture, allowing developers to:\n- Develop and test locally on their laptops\n- Deploy the same code to Snowflake without modifications\n- Maintain a smooth development workflow"
  },
  {
   "cell_type": "code",
   "id": "8f4baab3-acf5-419c-b6c2-633bb8971be4",
   "metadata": {
    "language": "sql",
    "name": "penguins_data",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "SELECT * FROM ST_ML_APP.DATA.PENGUINS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e3ee447-4c08-477a-900b-ec8c29f6ad3a",
   "metadata": {
    "name": "md_dp_details",
    "collapsed": false,
    "resultHeight": 248
   },
   "source": "### Data Preprocessing Steps\n1. Import SQL output to pandas DataFrame, you can refer to the cell name in Snowflake Notebooks in this case `penguins_data`\n2. Standardize column names to lowercase for consistency and easier reference\n3. Set appropriate data types for each column:\n   - Numeric columns: Convert to float64\n   - Text columns: Convert to string\n\nThe text is clear, concise, and properly structured with the correct heading level (##), numbered list, and nested bullet points. No changes are needed."
  },
  {
   "cell_type": "code",
   "id": "efbd843d-0ffb-4785-8be0-1bb2d47fd05c",
   "metadata": {
    "language": "python",
    "name": "py_prep_data",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "df = penguins_data.to_pandas()\n\n# for consistency and easiness let us change the column names to be of lower case\ndf.columns=df.columns.str.lower()\n\n## Set the columns to right data type\ndf['island'] = df['island'].astype('str')\ndf['species'] = df['species'].astype('str')\ndf['bill_length_mm'] = df['bill_length_mm'].astype('float64')\ndf['bill_depth_mm'] = df['bill_depth_mm'].astype('float64')\ndf['flipper_length_mm'] = df['flipper_length_mm'].astype('float64')\ndf['body_mass_g'] = df['body_mass_g'].astype('float64')\ndf['sex'] = df['sex'].astype('str')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17147ef3-62d4-45dc-aec9-adaa838c2056",
   "metadata": {
    "name": "md_st_exapander",
    "collapsed": false,
    "resultHeight": 436
   },
   "source": "\n### Streamlit Expander Widget 📂\n\nAn `st.expander` creates a collapsible section in your app that can be expanded/collapsed by clicking. It's useful for:\n- Hiding optional details or settings\n- Organizing long-form content\n- Creating FAQ-style interfaces\n- Showing additional visualizations on demand\n\n#### Key Features\n- Maintains a clean UI by hiding secondary content\n- Can contain any Streamlit elements (text, charts, inputs, etc.)\n- Default state can be set (expanded/collapsed)\n- Customizable label text\n\n📚 Documentation: https://docs.streamlit.io/library/api-reference/layout/st.expander"
  },
  {
   "cell_type": "code",
   "id": "8382cb6f-d738-4794-a20d-ee443d819510",
   "metadata": {
    "language": "python",
    "name": "st_show_raw_data",
    "collapsed": false,
    "resultHeight": 1076
   },
   "outputs": [],
   "source": "with st.expander(\"**Raw Data**\"):\n    df.columns = df.columns.str.lower()\n    \n    st.write(\"**X**\")\n    st.write(\"The input features that will use to build the model.\")\n    X_raw = df.drop(\"species\", axis=1)\n    X_raw\n\n    st.write(\"**y**\")\n    st.write(\"The target of our predicted model.\")\n    y_raw = df.species\n    y_raw",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed1ea63b-fca5-4704-8447-33af38515948",
   "metadata": {
    "name": "md_st_visualization",
    "collapsed": false,
    "resultHeight": 577
   },
   "source": "### Scatter Plot Visualization using Altair in Streamlit 📊\n\nAltair (powered by Vega-Lite) provides more customizable scatter plots than Streamlit's built-in charts. Perfect for the penguins dataset with features like:\n- Interactive tooltips with custom formatting\n- Layered visualizations\n- Color encoding by categorical variables\n- Dynamic filtering and zooming\n- Configurable axis and legend properties\n\n#### Key Advantages\n- Declarative grammar of graphics\n- Seamless integration with pandas DataFrames\n- Publication-quality aesthetics\n- Compositional layering system\n\n📚 Documentation:\n- Altair: https://altair-viz.github.io/user_guide/marks/scatter.html\n- Streamlit-Altair Integration: https://docs.streamlit.io/library/api-reference/charts/st.altair_chart\n\n*Note: Altair works natively with Streamlit using `st.altair_chart()`. No additional configuration needed.*"
  },
  {
   "cell_type": "code",
   "id": "884c6956-caef-4486-8d51-980abcd6fb67",
   "metadata": {
    "language": "python",
    "name": "st_data_visualization",
    "collapsed": false,
    "resultHeight": 437
   },
   "outputs": [],
   "source": "import altair as alt\n\nwith st.expander(\"Data Visualization\",expanded=True):\n   sp=alt.Chart(df).mark_circle().encode(\n     alt.X('bill_length_mm').scale(zero=False),\n     alt.Y('body_mass_g').scale(zero=False, padding=1),\n     color='species',\n   )\n\n   st.altair_chart(sp)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37c935a6-bc55-4359-bcac-451a72bf806d",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 826
   },
   "source": "\n### Interactive Widgets for Data Filtering 🎛️\n\nStreamlit provides several widgets to create dynamic, interactive filters for your data:\n\n#### Select Box (`st.selectbox`)\n- Dropdown menu for single selection\n- Perfect for categorical filters (e.g., penguin species)\n- Clean interface for limited options\n📚 [Select Box Documentation](https://docs.streamlit.io/library/api-reference/widgets/st.selectbox)\n\n#### Radio Button (`st.radio`)\n- Visual selection for mutually exclusive options\n- Great for 2-5 choices\n- More visible than dropdown menus\n📚 [Radio Button Documentation](https://docs.streamlit.io/library/api-reference/widgets/st.radio)\n\n#### Slider (`st.slider`)\n- Interactive range selection\n- Works with numbers, dates, and times\n- Supports single value or range selection\n- Ideal for numerical filters (e.g., bill length range)\n📚 [Slider Documentation](https://docs.streamlit.io/library/api-reference/widgets/st.slider)\n\n#### Sidebar Organization (`st.sidebar`)\nAll these widgets can be neatly organized in a collapsible sidebar using `st.sidebar`:\n- Keeps main content area clean\n- Creates intuitive filter panel\n- Automatically responsive\n- Perfect for filter controls and app navigation\n📚 [Sidebar Documentation](https://docs.streamlit.io/library/api-reference/layout/st.sidebar)\n\n*💡 Pro Tip: Using `with st.sidebar:` context manager keeps your sidebar code organized and readable. Very useful for standlone apps.*"
  },
  {
   "cell_type": "code",
   "id": "b07a2a93-2dc4-4b9a-a4b1-9adf0af9c574",
   "metadata": {
    "language": "python",
    "name": "st_input_features",
    "collapsed": false,
    "resultHeight": 633
   },
   "outputs": [],
   "source": "st.header(\"Input Features\")\n# Islands\nislands = df.island.unique().astype(str)\nisland = st.selectbox(\n    \"Island\",\n    islands,\n)\n# Bill Length\nmin, max, mean = (\n    df.bill_length_mm.min(),\n    df.bill_length_mm.max(),\n    df.bill_length_mm.mean().round(2),\n)\nbill_length_mm = st.slider(\n    \"Bill Length(mm)\",\n    min_value=min,\n    max_value=max,\n    value=mean,\n)\n# Bill Depth\nmin, max, mean = (\n    df.bill_depth_mm.min(),\n    df.bill_depth_mm.max(),\n    df.bill_depth_mm.mean().round(2),\n)\nbill_depth_mm = st.slider(\n    \"Bill Depth(mm)\",\n    min_value=min,\n    max_value=max,\n    value=mean,\n)\n# Filpper Length\nmin, max, mean = (\n    df.flipper_length_mm.min(),\n    df.flipper_length_mm.max(),\n    df.flipper_length_mm.mean().round(2),\n)\nflipper_length_mm = st.slider(\n    \"Flipper Length(mm)\",\n    min_value=min,\n    max_value=max,\n    value=mean,\n)\n# Body Mass\nmin, max, mean = (\n    df.body_mass_g.min(),\n    df.body_mass_g.max(),\n    df.body_mass_g.mean().round(2),\n)\nbody_mass_g = st.slider(\n    \"Body Mass(g)\",\n    min_value=min,\n    max_value=max,\n    value=mean,\n)\n# Gender\ngender = st.radio(\n    \"Gender\",\n    (\"male\", \"female\"),\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63c01a8f-ccfe-41d2-8404-7055561a6615",
   "metadata": {
    "name": "md_dataframe",
    "collapsed": false,
    "resultHeight": 114
   },
   "source": "### Display Input Features\nWe will use Streamlit's [data display elements](https://docs.streamlit.io/library/api-reference/data/st.dataframe) to showcase our input features. The `st.dataframe()` function provides an interactive table with sorting and filtering capabilities."
  },
  {
   "cell_type": "code",
   "id": "9d97ac23-af20-480d-8053-f01a4b448ca9",
   "metadata": {
    "language": "python",
    "name": "st_input_features_df",
    "collapsed": false,
    "resultHeight": 64
   },
   "outputs": [],
   "source": "data = {\n    \"island\": island,\n    \"bill_length_mm\": bill_length_mm,\n    \"bill_depth_mm\": bill_depth_mm,\n    \"flipper_length_mm\": flipper_length_mm,\n    \"body_mass_g\": body_mass_g,\n    \"sex\": gender,\n}\ninput_df = pd.DataFrame(data, index=[0])\ninput_penguins = pd.concat([input_df, X_raw], axis=0)\n\nwith st.expander(\"Input Features\"):\n    st.write(\"**Input Penguins**\")\n    input_df\n    st.write(\"**Combined Penguins Data**\")\n    input_penguins",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ffd1344-e95f-4b19-88ab-c523067cbc7f",
   "metadata": {
    "name": "md_dp_encode",
    "collapsed": false,
    "resultHeight": 200
   },
   "source": "### Data Prepration\n\nFor the data preparation step in this demo, we'll keep things straightforward and focus on:\n1. Encoding string features - converting text values into numbers that our ML model can understand\n2. Preparing the target variable - ensuring our prediction target is properly encoded\n\nThis will be a minimal demonstration without additional preprocessing steps like feature scaling, handling missing values, or feature engineering. "
  },
  {
   "cell_type": "code",
   "id": "201896f7-0d16-4314-afc1-85c4cc5e880e",
   "metadata": {
    "language": "python",
    "name": "py_model_data_prep",
    "collapsed": false,
    "resultHeight": 64
   },
   "outputs": [],
   "source": "X_encode = [\"island\", \"sex\"]\ndf_penguins = pd.get_dummies(input_penguins, prefix=X_encode)\nX = df_penguins[1:]\ninput_row = df_penguins[:1]\n\n## Encode Y\ntarget_mapper = {\n    \"Adelie\": 0,\n    \"Chinstrap\": 1,\n    \"Gentoo\": 2,\n}\n\ny = y_raw.apply(lambda v: target_mapper[v])\n\nwith st.expander(\"Data Preparation\"):\n    st.write(\"**Encoded X (input penguins)**\")\n    input_row\n    st.write(\"**Encoded y**\")\n    y",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f17782f0-8c05-4908-851c-4acf6e6fcede",
   "metadata": {
    "name": "md_train_predict",
    "collapsed": false,
    "resultHeight": 535
   },
   "source": "### Model Training and Prediction\n\nFor this final step, we'll use RandomForestClassifier - an ensemble learning method that operates by constructing multiple decision trees during training and outputs the class that is the mode of the classes predicted by individual trees. We'll display the progress and results using Streamlit's container and progress components for a better user experience, followed by a success message showing the prediction results.\n\nRandomForest is a good choice for our demonstration as it:\n- Handles both numerical and categorical features well\n- Provides feature importance rankings\n- Is less prone to overfitting compared to single decision trees\n- Requires minimal hyperparameter tuning to get reasonable results\n\n**References:**\n* [Scikit-learn RandomForestClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n* [Scikit-learn Ensemble Methods Guide](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n* [User Guide: Forest of randomized trees](https://scikit-learn.org/stable/modules/forest.html)\n* [Streamlit Container API](https://docs.streamlit.io/library/api-reference/layout/st.container)\n* [Streamlit Progress and Status API](https://docs.streamlit.io/library/api-reference/status/st.progress)\n* [Streamlit Success Message](https://docs.streamlit.io/library/api-reference/status/st.success)"
  },
  {
   "cell_type": "code",
   "id": "a610ccd1-d748-44cf-b9c9-e03e11baac1d",
   "metadata": {
    "language": "python",
    "name": "st_train_predict",
    "collapsed": false,
    "resultHeight": 260
   },
   "outputs": [],
   "source": "with st.container():\n    st.subheader(\"**Prediction Probability**\")\n    ## Model Training\n    rf_classifier = RandomForestClassifier()\n    # Fit the model\n    rf_classifier.fit(X, y)\n    # predict using the model\n    prediction = rf_classifier.predict(input_row)\n    prediction_prob = rf_classifier.predict_proba(input_row)\n\n    # reverse the target_mapper\n    p_cols = dict((v, k) for k, v in target_mapper.items())\n    df_prediction_prob = pd.DataFrame(prediction_prob)\n    # set the column names\n    df_prediction_prob.columns = p_cols.values()\n    # set the Penguin name\n    df_prediction_prob.rename(columns=p_cols)\n\n    st.dataframe(\n        df_prediction_prob,\n        column_config={\n            \"Adelie\": st.column_config.ProgressColumn(\n                \"Adelie\",\n                help=\"Adelie\",\n                format=\"%f\",\n                width=\"medium\",\n                min_value=0,\n                max_value=1,\n            ),\n            \"Chinstrap\": st.column_config.ProgressColumn(\n                \"Chinstrap\",\n                help=\"Chinstrap\",\n                format=\"%f\",\n                width=\"medium\",\n                min_value=0,\n                max_value=1,\n            ),\n            \"Gentoo\": st.column_config.ProgressColumn(\n                \"Gentoo\",\n                help=\"Gentoo\",\n                format=\"%f\",\n                width=\"medium\",\n                min_value=0,\n                max_value=1,\n            ),\n        },\n        hide_index=True,\n    )\n\n# display the prediction\nst.subheader(\"Predicted Species\")\nst.success(p_cols[prediction[0]])\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05b25160-ff9c-4d04-a1ae-939dc28c30b6",
   "metadata": {
    "name": "md_note",
    "collapsed": false,
    "resultHeight": 309
   },
   "source": "\n⚠️ **Important Note:**\n* When changing input features, cells don't automatically re-run\n* After modifying `st_input_features`, you need to manually run these cells in sequence:\n  1. `st_input_features_df` - Updates the features DataFrame\n  2. `py_model_data_prep` - Prepares data for model training\n  3. `st_train_predict` - Trains model and shows prediction\n\nHere is execution of cells flow:\n\n`Change inputs[st_input_features]` → `Update DataFrame[st_input_features_df]` → `Prepare ML data[py_model_data_prep]` → `Train & predict[st_train_predict]`\n                    "
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7c05-603d-4a91-9a4e-87271ca6aad9",
   "metadata": {
    "name": "md_summary",
    "collapsed": false,
    "resultHeight": 672
   },
   "source": "## Summary and Further Reading\n\nThroughout this course, we've seen how Snowflake Notebooks and Streamlit work together to create powerful, interactive machine learning applications. This combination offers several advantages:\n\n1. **Unified Development Environment**: Snowflake Notebooks provide a seamless environment for data preparation, model development, and testing, all within the Snowflake ecosystem.\n\n2. **Interactive User Interfaces**: Streamlit enables us to transform our machine learning models into user-friendly applications, making complex analytics accessible to non-technical users.\n\n3. **Scalable Processing**: By leveraging Snowflake's computational power, our applications can handle large-scale data processing without compromising performance.\n\n4. **Real-time Analytics**: The integration allows for real-time data updates and model predictions, making our applications more dynamic and valuable for business decisions.\n\n## Further Reading\n\n- [Streamlit in Snowflake](https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit) - Learn more about building interactive data applications\n- [Snowpark Python DataFrames](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes) - Deep dive into data manipulation techniques\n- [Snowflake ML](https://docs.snowflake.com/en/developer-guide/snowflake-ml/snowpark-ml) - Explore advanced machine learning capabilities\n- [Snowflake Notebooks](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks) - Master the notebook environment for development\n- [Snowflake Quickstarts](https://quickstarts.snowflake.com/) - Get hands-on experience with guided tutorials and examples\n\nHappy building!"
  }
 ]
}